Let us use Figure~\ref{rel-graph} to illustrate the benefit of {\em
modeling the variable name generation/recovery problem as predicting
the missing features} using GCNmf~\cite{GCNmf}. A variable with a
minified name is modeled as a nodel in our graph, e.g., $r$, $q$, and
$d$.  The prior works based on machine translation (e.g.,
JSNaughty~\cite{JSNaughty2017}) aiming to translate the minified code
to the orginal code, face an issue of different naming schemes used
by different minification tools. For example, the
variable \code{range} might become $r_1$, instead of $r$, by a
different minification tool or by alpha-renaming. In {\tool}, the
minified names themselves do not play a crucial role in deciding the
original names as in prior work. They help in recognizing the
occurrences of the same variable. In our graph $G$, {\tool}
considers a node for a minified variable as a placeholder with a
missing feature that it aims to fill in.  In prediction, GCNmf will
create the vectors $v_r$, $v_q$, and $v_d$ for the nodes. During the
convolution process, those vectors are automatically updated based on
the neighboring node features and the relations. After convolution,
the final vectors will be fed into the GRU decoder for variable name
prediction.

%We use Figure~\ref{rel-graph} as an example to explain how our
%variable name generation model works. Because when we use different
%code minification tools to process the same source code, the minified
%variable names could be different. In this case, the minified variable
%names do not have fixed meanings in different codes with varying
%minification tools.
%So in our variable name generation model, when the generated combined
%graph comes in, we regard the node $n_r, n_q, n_d$ with the minified
%variable names such as $r, q, d$ in Figure~\ref{rel-graph} as the
%nodes with missing node features. After feeding the graph into the
%GCNmf model, it will randomly create representation vectors $v_r, v_q,
%v_d$ for nodes $n_r, n_q, n_d$ at first. During the convolution
%process, the representation vectors $v_r, v_q, v_d$ will be
%automatically updated based on the neighbor node features and the
%connection relationships. When the convolution process is finished,
%the final stage of representation vectors $v'_r, v'_q, v'_d$ for nodes
%$n_r, n_q, n_d$ are fed into the GRU decoder to pict minified
%variable names.

%At the same time, the final representation vectors for each node can
%be used in the type generation model for the type prediction.
