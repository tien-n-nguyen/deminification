\section{Introduction}
\label{intro:sec}

Understandability is an important software quality. Software
developers have to spend a significant portion of their efforts in
reading and comprehending source code. Beside the documentation,
meaningful names for variables and types are crucial for developers in
quickly grasping the essence of the code. Software organizations have
much emphasized on naming conventions and coding standards to ensure
meaningful variable names in source
code~\cite{barr-codeconvention-fse14}.

%An important aspect of program understanding is the names of the
%identifiers~used in the source code~\cite{sutton-fse15}. Meaningful
%identifiers help developers tremendously in quickly grasping the
%essence of the~code. Thus, naming conventions are strongly emphasized
%on prescribing how to choose meaningful variable names in coding
%standards~\cite{barr-codeconvention-fse14}.  These principles also
%apply to Web development.

%In modern Web development, program understanding plays an equally
%important role.

In modern software development, some technologies require the exposure
of source code, e.g., in the client-server paradigm, the client code
need to executed in the client side. To avoid the exposure, the source
code is obfuscated and/or minified such that the variable names are
replaced with short, opaque, and meaningless names. The minification
of the variable names hides the business logics from the readers while
maintaining the essence of the code. {\color{blue}{Code minification
    is a code protection mechanism that at least slows down those who
    have bad intentions because program code is a valuable asset to
    companies. In other cases, code minification improves rendering
    times because of payload size.}}

%For Web programming, the JS files are also more quickly loaded from
%the servers for better performance.


%Web technologies and programming languages require the exposure of
%source code to Web browsers in the client side to be executed
%there. To avoid such exposure, the source code such as JavaScript (JS)
%files are often obfuscated in which the variable names are minified,
%\ie the variable names are replaced with short, opaque, and
%meaningless names. The intention has two folds. First, it makes the JS
%files smaller and thus is quickly loaded for better performance.
%Second, minification diminishes code readability to hide business
%logics from the readers, while maintaining the program semantics.
%
%\textbf{would be better to focus on the reason of making files small. In Web,JS code is transfer over the Internet from server to client to be executed there. Due to the limit in the bandwidth or the memory capacity of the
%(mobile) device, JS code is normally minified to reduce its size. When being minified, the names are replaced ...}
%

For better code readability and understandability, especially when the
original source code is unavailable, there is a natural need to
automatically recover the minified code with meaningful variable
names. With the recovered names and types, the source code will be
more comprehensible and accessible for maintenance activities such
as code review, analysis, enhancement, and reuse.

%Due to those reasons, there is a natural need to automatically recover
%the minified code with meaningful variable names. When the original
%code is not available, with such recovery, the minified JS code will
%be made accessible for code~compre-\\hension as well as other maintenance
%activities such as~code review, reuse, analysis, and
%enhancement.

%Recognizing that need, researchers have been introducing the
%automatically recovering tools for variable names in JS code.


Several automated approaches have been proposed to automatically
recover the names of the variables in the minified source code.  The
approaches can be broadly classified into two directions: {\em
  information retrieval}, {\em statistical learning}, and {\em machine
  learning}.  JSNeat~\cite{icse19} follows an information retrieval
(IR) approach to recover names by searching for them in a large corpus
of open-source JS code. JSNeat integrates three types of contexts to
match a variable in given minified code against the corpus including
%1) the context of
1) the properties and roles of the variable, 2)
%the context of that variable and
its relations with other variables under
recovery, and 3)
%the context of
the task of the function to which the variable contributes.  Despite
its successes, due to the inherent limitation of the information
retrieval direction, JSNeat {\em cannot generate a new variable name}
that was not encountered in the corpus.

JSNice~\cite{JSNice2015}, following a statistical direction, is an
automatic variable name recovery approach that represents the program
properties and relations among program entities in JavaScript (JS)
code as dependence graphs. JSNice~\cite{JSNice2015} uses a
probabilistic model with the dependency network including
variables and surrounding program entities. It formulates the problem
of variable name recovery as the structured prediction via conditional
random fields (CRFs)~\cite{JSNice2015}. Unfortunately, it still has
low accuracy.
%
In contrast, JSNaughty~\cite{JSNaughty2017} formulates
%the variable name recovery
that problem as a {\em statistical machine translation} from
minified code to the recovered code. However, its
%JSNaughty~\cite{JSNaughty2017}'s
phrase-based translation approach cannot capture well the relations
among the variables that need to be recovered, leading to low
effectiveness.

%We use multi-tasking framework to do the name prediction and the type
%predicting at the same time.

%We consider the code structure among the code

%We consider the context (different types of relation) between
%variables that need to be recovered => EGCN

In this work, we present {\tool}, a deep learning (DL)-based variable
name recovery approach. We focus mainly on variable name recovery and
address it as part of the dual-task learning~between the variable name
prediction model and the type prediction model. {\em Correct learning
  of one model can benefit for the learning of the other and vice
  versa} due to the naturalness of names in source
code~\cite{hindle-icse12}.
%
Except for a few un-important variables (e.g., running variables in a
loop or temporary variables), the majority of the variables carry some
contextual meaning toward achieving the task intended in the current
function.
%
{\em The names chosen for such a variable in the original code should
  be natural (unsurprising) with respect to its type}.
%The rationale is that {\em the name chosen for a variable in the
%original code should be natural (unsurprising) with respect to the
%type of that variable}.
For example, for easy comprehension, a variable of the type
\code{Offset} might have a name relevant to the notion of {\em offset}
or its abbreviations, e.g., \code{startOffset}, \code{endOffset}, etc.
Similarly, if a model learns the name of a variable, its type should
be in accordance with the name. For example, if a model recovers the
name of a variable as {\em count} or {\em index}, its type might
likely be of \code{int} or \code{Integer}.

%
%For example, if a variable of the type \code{Student} should not have
%a name that is completely irrelevant to the notion
%``student''.
%%and follow naming conventions

Exploring this duality can provide useful constraints to predict both
the variable names and their types.  To build the variable name
prediction model and the type prediction model, we leverage Graph
Convolution Network - Missing Features (GCNmf)~\cite{GCNmf} to model
different kinds of {\em relations/dependencies among the variables and
  among the types}. We formulate {\bf the name recovery problem as the
  predicting the missing features} in GCNmf. To propagate the mutual
impact of type learning and name learning, we apply a dual-task
learning mechanism between the two models.
%
With the philosophy that {\em ``Tell Me Your Friends, I'll Tell You Who
  You Are''}, {\tool} decides a variable's name by learning at once
the names of the variables connecting to one another.
%
%We use the uncertainty weighted multi-task loss as the multi-task
%learning loss function and use the maximum of the accuracy score from
%two tasks as the training target.
Our DL-based model is expected to have better predictive power than
the CRF in JSNice and SMT in JSNaughty, especially in predicting the
missing features when considering the relations among variables and
types.

We have conducted experiments to evaluate {\tool} in both name
recovery and type prediction on a Python dataset with +180k methods
and {\color{blue}{and a JavaScript (JS) dataset with 322k
    files}}. For variable name prediction, in 76.7\% and
{\color{blue}{79.9\%}} of the cases {\color{blue}{in Python and JS
    code respectively}}, {\tool} can predict correctly the variables'
names with a single suggested name. In 82\% {\color{blue}{and 87.3\%}}
of the cases in Python {\color{blue}{and JS code}}, the correct names of local variables are in the top-5 candidate suggested lists. For Python code, {\tool}
relatively improves 40.7\%, 28.7\%, and 15.3\% top-1 accuracy over the
state-of-the-art variable name recovery approaches
JSNice~\cite{JSNice2015}, JSNaughty~\cite{JSNaughty2017}, and
JSNeat~\cite{icse19}, respectively.
%
{\color{blue}{For JS code, the relative improvements over those baselines are 46.6\%,
    34.0\%, and 5.4\%, respectively.}} For variable type prediction
in Python, in 79\% of the cases, {\tool} can predict correctly the
types with a single predicted type. Top-5 accuracy for type prediction
is 88\%. It relatively improves 14.5\%--51.9\% in top-1 accuracy and
22.2\%--46.6\% in top-5 accuracy over the state-of-the-art type
prediction approaches HiTyper~\cite{HiTyper-icse22},
Type4Py~\cite{Type4Py-icse22}, Typilus~\cite{typilus-pldi20}, and
TypeWriter~\cite{typewriter-fse20}.

In brief, the contributions of this paper includes:

1. {\bf {\tool}}: a Deep Learning (DL)-based approach to recover
variable names for minified code with type inference in dual-task
learning. {\tool}'s type inference can be used for regular code.

2. An extensive evaluation and analysis on {\tool}'s
accuracy.
%(Data and code is available in a website~\cite{deminify-website});

%1. {\bf {\tool}}: a Deep Learning (DL)-based approach to recover both
%variable names and types in a dual-task learning;

%2. An extensive comparative evaluation and analysis on {\tool}'s
%accuracy (Data and code is available in a
%website~\cite{deminify-website});

3. A novel formulation of variable name and type recovery in
minified code as a missing-feature, graph-based neural network.
