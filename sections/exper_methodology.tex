\input{sections/dataset.tex}

\subsection{Experimental Methodology}
\label{empirical-method}

\subsubsection{RQ1. Comparative Study on Variable Name Prediction.}

{\em Baselines.} We compared {\tool} against the state-of-the-art variable name prediction approaches: JSNeat~\cite{tran2019recovering}, JSNice~\cite{JSNice2015}, and JSNaughty~\cite{JSNaughty2017} 

\textit{Procedure.} We took all the methods in the ManyTypes4Py dataset and ran the pyminifier \cite{pyminifier} tool that is used for variable name minification to minify all the variables in each method. Then we use 80\% of the methods as the training dataset, 10\% of the methods as the tuning dataset, and 10\% of the methods as the testing dataset for all baselines and {\tool}.

Because all of these three baselines are designed for JavaScript, we reimplement them based on their source code and design to make them can work on python. To keep the baselines in their original track, we keep all parameters the same as in their original research studies.

We tuned {\tool} with autoML~\cite{NNI} for the following key hyper-parameters to have the best performance: (1) Epoch size (100, 150, 200); (2) Batch size (64, 128, 256); (3) Learning rate (0.001, 0.005, 0.010); (4) Vector length of feature embeddings and its
output (32, 64, 128); (5) Number of GCN layers (4, 8, 16). 

\subsubsection{RQ2. Comparative Study on Variable Type Prediction.}

{\em Baselines.} We compared {\tool} against the state-of-the-art variable type prediction approaches: HiTyper~\cite{type-graph-icse22}, Type4PY~\cite{mir2021type4py}, Typilus~\cite{allamanis2020typilus}, Ivanov et al.\cite{ivanov21predicting}, and TypeWriter~\cite{typewriter-fse20}

\textit{Procedure.} With all variable types information summarized in the ManyTypes4Py dataset, we directly used the variable types information for variables in each method for all approaches in this RQ. Because {\tool} is a dual-learning approach, to make {\tool} work correctly, we took the same data splitting for training, tuning, and testing dataset as in RQ1 for all baselines and {\tool} in this RQ.

Because all baselines in this RQ are designed for python, we directly used the original implementation for each baseline and kept all parameters the same as in their original research studies. And we tuned {\tool} with autoML~\cite{NNI} for the same key hyper-parameters to have the best performance as in RQ1.

\subsubsection{RQ3. Sensitivity Analysis}

We built the variants of {\tool} by changing its important components to check the influence of each component on both the variable name and type prediction results. These variants of {\tool} include: 1. using label-GCN (LGCN) as the variable type prediction model and using GCN with missing features (GCNmf) as the variable name prediction model on relation graph (RG); 2. using edge-enhanced GCN (EEGCN) as the variable type prediction model and using GCNmf as the variable name prediction model on code property graph (CPG); 3. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on RG; 4. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on type dependency graph (TDG); 5. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on the combined graph (CG).

\subsubsection*{Evaluation Metrics}

For the variable name prediction task, we follow the existing research \cite{tran2019recovering} to calculate the prediction accuracy on local variables and all variables. And for the variable type prediction task, following the previous work \cite{type-graph-icse22}, we use two metrics, Exact Match and Match to Parametric, on top-1, top-3, and top-5 for evaluation. These two metrics compute the ratio of results that: 1) Exact Match: completely matches human annotations. 2) Match to Parametric: satisfy exact match when ignoring all the type parameters. (e.g., Dict[int, str] and Dict[int, int] are considered as matched under Match to Parametric metric.)

