\subsection{Experimental Methodology}
\label{empirical-method}

\subsubsection{RQ1. Comparative Study on Variable Name Prediction\\}
{\em Baselines.} We compared {\tool} against the state-of-the-art
variable name recovery approaches for minified code including
JSNeat~\cite{tran2019recovering}, JSNice~\cite{JSNice2015}, and
JSNaughty~\cite{JSNaughty2017}.

\textit{Procedure.} We took all the methods in that Python dataset
and used Pyminifier to produce the minified code with variable name
mininification. We randomly split all the methods into 80\%, 10\%,
10\% in which 80\% of the methods as the training dataset, 10\% of the
methods as the tuning dataset, and 10\% of the methods as the testing
dataset for all baselines and {\tool}.

%We took all the methods in the ManyTypes4Py dataset and ran the pyminifier \cite{pyminifier} tool that is used for variable name minification to minify all the variables in each method. Then we use 80\% of the methods as the training dataset, 10\% of the methods as the tuning dataset, and 10\% of the methods as the testing dataset for all baselines and {\tool}.

{\em Parameter Tuning.} We tuned {\tool} with autoML~\cite{NNI} for
the following key hyper-parameters to have the best performance: (1)
Epoch size (100, 150, 200); (2) Batch size (64, 128, 256); (3)
Learning rate (0.001, 0.005, 0.010); (4) Vector length of feature
embeddings and its output (32, 64, 128); (5) Number of GCN layers (4,
8, 16).

\subsubsection{RQ2. Comparative Study on Variable Type Prediction\\}

{\em Baselines.} We compared {\tool} against the state-of-the-art
  variable type prediction approaches: Ivanov {\em et
  al.}~\cite{ivanov21predicting}, Typilus~\cite{typilus-pldi20},
  TypeWriter\cite{typewriter-fse20}, Type4Py~\cite{Type4Py-icse22},
  and HiTyper~\cite{HiTyper-icse22}.

%HiTyper~\cite{type-graph-icse22}, Type4PY~\cite{mir2021type4py},
%Typilus~\cite{allamanis2020typilus}, Ivanov et
%al.\cite{ivanov21predicting}, and TypeWriter~\cite{typewriter-fse20}.

We did not compare with Xu {\em et al.}~\cite{xu-fse16},
DeepTyper~\cite{DeepTyper-fse18}, NL2Type~\cite{nl2type-icse19},
LAMBDANET~\cite{LambdaNet-iclr20}, OptTyper~\cite{optyper20}, and
TypeBERT~\cite{typeBERT-fse21}, because in its paper, Type4Py has been
compared to show the better performance than them, and we compared
with Type4Py.


\textit{Procedure.} With all variable types information summarized in the ManyTypes4Py dataset, we directly used the variable types information for variables in each method for all approaches in this RQ. Because {\tool} is a dual-learning approach, to make {\tool} work correctly, we took the same data splitting for training, tuning, and testing dataset as in RQ1 for all baselines and {\tool} in this RQ.

Because all baselines in this RQ are designed for python, we directly used the original implementation for each baseline and kept all parameters the same as in their original research studies. And we tuned {\tool} with autoML~\cite{NNI} for the same key hyper-parameters to have the best performance as in RQ1.

\subsubsection{RQ3. Sensitivity Analysis}

We built the variants of {\tool} by changing its important components to check the influence of each component on both the variable name and type prediction results. These variants of {\tool} include: 1. using label-GCN (LGCN) as the variable type prediction model and using GCN with missing features (GCNmf) as the variable name prediction model on relation graph (RG); 2. using edge-enhanced GCN (EEGCN) as the variable type prediction model and using GCNmf as the variable name prediction model on code property graph (CPG); 3. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on RG; 4. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on type dependency graph (TDG); 5. using EEGCN as the variable type prediction model and using GCNmf as the variable name prediction model on the combined graph (CG).

\subsubsection*{Evaluation Metrics}

For the variable name prediction task, we follow the existing research \cite{tran2019recovering} to calculate the prediction accuracy on local variables and all variables. And for the variable type prediction task, following the previous work \cite{type-graph-icse22}, we use two metrics, Exact Match and Match to Parametric, on top-1, top-3, and top-5 for evaluation. These two metrics compute the ratio of results that: 1) Exact Match: completely matches human annotations. 2) Match to Parametric: satisfy exact match when ignoring all the type parameters. (e.g., Dict[int, str] and Dict[int, int] are considered as matched under Match to Parametric metric.)

